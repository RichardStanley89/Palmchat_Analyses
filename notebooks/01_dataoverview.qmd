---
title: "dataoverview"
format: html
editor: visual
---

# Introduction

## Palmchat Banding Data:

The Palmchat Banding Data spreadsheet contains data on the individual birds that were captured, banded, and released at my field site in Punta Cana in the Dominican Republic. This spreadsheet is for keeping track of the identity of individual birds (which have unique numbers and band color combinations), as well as the characteristics of each bird (eg. body size, male or female, age where known, and other morphological characteristics). Because many different field assistants contributed to collecting this data, there are some differences in how the data were collected over different time periods (eg. Sometimes it was possible to age birds in the field based on molt patterns, but not every observer could do this, and sometimes their age scoring systems were slightly different)

```{r setup, message=FALSE}
# Load the libraries
library("tidyverse")
library("dplyr")
library("asnipe")
library("igraph")
library("statnet")

```

We are loading the three datasets here, and taking a quick look. The `Dyads` data is the sightings

```{r load_data}
rawdata <- read.csv("../rawdata/PalmchatBandingDataAll.csv")
head(rawdata)
edges <- read.csv("../rawdata/PalmchatDyads.csv")
head(edges)
```

## Cleaning

### Uncertainty removal

First we want to filter out the dyads with unbanded birds, which is around \~260 observations

```{r}
unbanded <- edges %>% 
  filter(str_detect(Individual1, "U") | str_detect(Individual2, "U") |
         str_detect(Individual1, "juv") | str_detect(Individual2, "juv"))

slice_head(unbanded, n=10)  

nrow(unbanded)
```

J: are these observations with unbanded individuals evenly spread over the study period? Or are there any specific days that are overrepresented?

```{r}
unbanded %>% 
  mutate(Date = lubridate::ymd(Date)) %>% 
  ggplot(aes(x = Date)) +
  geom_bar()
```

We have \~800 observations now, and have to remove the ones with uncertainty

```{r}
edges %>% 
  anti_join(unbanded)
```

Interesting that some of the uncertain id ones also have an unbanded, reducing data loss.

```{r}
uncertain_id <- edges %>% 
  filter(str_detect(Individual1, "\\?") | str_detect(Individual2, "\\?"))
head(uncertain_id)

```


Now we can get the clean_edges by removing the unbamded, uncertain id and juveniles unbanded

```{r clean_edges, message=FALSE}

clean_edges <- edges %>% 
  anti_join(unbanded) %>% 
  anti_join(uncertain_id)
```

## Fix Bands

```{r}
# Step 1: identify the individual ids
band_ids <- unique(c(clean_edges$Individual1, clean_edges$Individual2)) %>% 
  str_trim()

# Step 2: separate the ones that have a missing band
ids_with_dash <- str_subset(band_ids, "-")
ids_with_dash

# W-WM this one is a true id because they ran out of bands
# remove from list
ids_with_dash <- ids_with_dash[ids_with_dash != "W-WM"]
ids_with_dash

# Step 3: identify potential matches:
create_pattern <- function(id) {
  str_replace_all(id, "-", ".")
}

patterns <- sapply(ids_with_dash, create_pattern)

# Step 3: Use str_detect to find all matching IDs
matches <- sapply(patterns, function(pattern) {
  matched_ids <- str_subset(band_ids, pattern)
  return(matched_ids)
})

names(matches) <- ids_with_dash
print(matches)

```
Those seem harder to resolve. Maybe we can filter by location? 


```{r}
# Double check that there are only two sites:
unique(clean_edges$Site)

# Separate the dataset based on the 'site' variable
clean_edges_hacienda <- clean_edges %>% filter(Site == "Hacienda")
clean_edges_corales <- clean_edges %>% filter(Site == "Corales")

# Repeat process for both:
get_ids_with_dash <- function(clean_edges_subset) {
  # Step 1: Identify the individual IDs
  band_ids <- unique(c(clean_edges_subset$Individual1, clean_edges_subset$Individual2)) %>%
    str_trim()
  
  # Step 2: Separate the ones that have a missing band
  ids_with_dash <- str_subset(band_ids, "-") %>%
    .[. != "W-WM"]  # Remove the known valid ID
  
  band_ids_no_dash <- setdiff(band_ids, ids_with_dash)
  
  # Step 3: Identify potential matches
  create_pattern <- function(id) {
    str_replace_all(id, "-", ".")
  }
  patterns <- sapply(ids_with_dash, create_pattern)
  
  # Step 4: Use str_detect to find all matching IDs
  matches <- sapply(patterns, function(pattern) {
    matched_ids <- str_subset(band_ids_no_dash, pattern)
    return(matched_ids)
  })
  names(matches) <- ids_with_dash
  
  return(
    list(ids_with_dash = ids_with_dash,
         matches = matches)
  )
  
}

# Process each subset
cat("Hacienda \n")
get_ids_with_dash(clean_edges_hacienda)

cat("Corales \n")
get_ids_with_dash(clean_edges_corales)
```
I don't think we can resolve these uncertain banding patterns. Let's check how much data we lose if we take them out:

```{r}
ids_with_dash

# it's just 1 observation here
clean_edges_hacienda %>% 
  filter(Individual1 %in% ids_with_dash | Individual2 %in% ids_with_dash)

# It's 18 observations here
clean_edges_corales %>% 
  filter(Individual1 %in% ids_with_dash | Individual2 %in% ids_with_dash)

# We can fix one of these:
# "Y-NM" => "YBNM"

clean_edges %>% 
  mutate(Individual1 = ifelse(Individual1 == "Y-NM", "YBNM", Individual1),
         Individual2 = ifelse(Individual1 == "Y-NM", "YBNM", Individual2)) -> clean_edges

# and also wondering if the "Y-BM" could be a misread of "YBNM"
# It's only 2 observations
clean_edges %>% 
  filter(Individual1 == "Y-BM" | Individual2 == "Y-BM")
```

For the sake of certainty, for now, let's just remove the unresolved observations. Unfortunately, for the corales data, some of the ones we are removing also show as group interactions, so I don't know if there is a way we can keep that data. 

```{r}
# We get 638 observations here
clean_edges_no_dash <- clean_edges %>% 
  filter(!(Individual1 %in% ids_with_dash | Individual2 %in% ids_with_dash))

# clean_edges %>% 
#   filter(Individual1 %in% ids_with_dash | Individual2 %in% ids_with_dash)

clean_edges_no_dash %>% 
  count(Site)
```

## Fix edge list for consistency in the order

We want to make sure that order is consistent (eg. BSY--YGG vs. YGG--BSY, which represent the same interaction, should always be in the same order to avoid confusion. Because we have the individuals with same band pattern, we also want to do this by site, and removing any observations that have missing bands. 

```{r}

clean_edges_no_dash %>% 
  group_by(Site) %>% 
  rowwise() %>%
  mutate(dyad_id = paste(sort(c(Individual1, Individual2)), collapse = "-"), 
         .after = Individual2) %>%
  ungroup() %>% 
  # Check the unique dyads
  count(dyad_id)

```

# Attributes

Let's take a look at the attributes associated with each bird, in the raw banding data: Clean out the recaptures first

```{r}
# There is an error in the banding process here
# These are new captures but accidentally got the same color combination as a previous new bird

rawdata %>% 
  filter(Status == "N") %>% 
  anti_join(
    rawdata %>% distinct(Colors, .keep_all = TRUE) 
  )

```

These are the banding errors. Which we can probably keep since they are in different sites. So, recommendation, keep in mind when doing the network analysis, to do it by site.


```{r}
rawdata %>% 
  filter(Colors %in% c("GBBM", "RBBM"), Status == "N") %>% 
  select(Location, Colors)
```

Based on this, we create the attribute table to keep the repeat birds since they are from different locations.

```{r}
attributes <- rawdata %>% 
  filter(Status == "N")
```

```{r}

# let's get rid of the columns we're not interested in, just to make things tidier:

attributes_clean <- attributes %>% 
  select(
    Year, Month, Day, Colors, Molecular_Sex, Body_Mass, UNFLT_Wing, 
    Age, Entrance_affiliation_based_on_videos_2024, Nest_affiliation..2024.
    ) %>% 
  janitor::clean_names()

head(attributes_clean)

```




```{r}

# Here, I will create a "dyadID," which gives each dyad the same name,
# regardless of the order. This is solely for the purpose of fixing the above issue:

#also only using the edges without missing bands 
clean_edges_no_dash$dyad_id <- apply(clean_edges_no_dash[,c("Individual1","Individual2")], 1, FUN= function(x) paste(sort(x), collapse="_"))

clean_edges$dyad_id

dyads<- data.frame(edges$dyad_id)

```

```{r}



# calculate edge weights by aggregating. This will collapse the repeat network
# ties, and add a column that represent the number of repeats (edge weight):

edges_with_weights <- aggregate(list(weight=rep(1,nrow(dyads))), dyads, FUN=sum)

# separate the dyads into separate columns again so we have an edge list,
# just like we started with:

edges <- edges_with_weights  %>% separate_wider_delim(edges.dyad_id, "_", names = c("col1","col2"))

# create a network object in statnet

network <- network(edges,matrix.type="edgelist", directed=FALSE)

list.edge.attributes(network)
as.edgelist(network)
get.edge.attribute(network, "weight")
list.vertex.attributes(network)

# 

### Now we need to add some other node attribute data to the network: eg. the Nest
# affiliation of each bird in the network, body mass, etc. However, the order
# of the data in the attributes data set differs from the order in the network.
# Let's work on this issue so we have a properly labeled network:

# line up vertex attributes in the same order as the network vertex data in statnet:

# let's double check what the order is in the network:
get.vertex.attribute(network,"vertex.names")

# first, create a column with the same names in each dataset so you can perform a join:


df <- mutate(attributes, col1 = Colors)
df2 <- data.frame(col1 = network.vertex.names(network))


# now, perform a join to get them in the right order.
# As a result of this, the attributes will now be in the same
# order as the nodes in the network:

dfjoined <- left_join(df2, df, by ="col1")

str(dfjoined$Nest_affiliation..2024.)
length(dfjoined$Nest_affiliation..2024.)

# now, add these node attributes to the statnet network.

# First, add the attribute "NestID"

set.vertex.attribute(network, "NestID", dfjoined$Nest_affiliation..2024.)

get.vertex.attribute(network,"NestID")

# assess the length and number of NAs in the new network attribute, NestID

sum(is.na(dfjoined$Nest_affiliation..2024.))
length(dfjoined$Nest_affiliation..2024.)
network.size(network)

# add the attribute "EntranceID"

set.vertex.attribute(network,"EntranceID", dfjoined$Entrance_affiliation_based_on_videos_2024)

get.vertex.attribute(network,"EntranceID")

list.vertex.attributes(network)
list.edge.attributes(network)

vertexnames <- get.vertex.attribute(network,"vertex.names")
vertexnames


# add the attribute "Sex"

set.vertex.attribute(network,"Sex", dfjoined$Molecular_Sex)

get.vertex.attribute(network,"Sex")



```

```{r}
# let's also load the data in group format, as a list (since this is the input
# preferred by the "asnipe" package, and it preserves information about group interactions)


groupdata <- read.csv("../rawdata/PalmchatGroups.csv")

# let's filter out sightings with ID uncertainty:

groupdata <-  groupdata %>% filter(!str_detect(groupdata$Individual1, "\\?"))
groupdata <-  groupdata %>% filter(!str_detect(groupdata$Individual2, "\\?"))
groupdata <-  groupdata %>% filter(!str_detect(groupdata$Individual3, "\\?"))
groupdata <-  groupdata %>% filter(!str_detect(groupdata$Individual4, "\\?"))
groupdata <-  groupdata %>% filter(!str_detect(groupdata$Individual5, "\\?"))
groupdata <-  groupdata %>% filter(!str_detect(groupdata$Individual6, "\\?"))

# and sightings with unbanded birds

groupdata <-  groupdata %>% filter(!str_detect(groupdata$Individual1, "U"))
groupdata <-  groupdata %>% filter(!str_detect(groupdata$Individual2, "U"))


# now let's put this data into a list format for the asnipe package:


groups <- groupdata %>% select(Individual1, Individual2, Individual3, Individual4,
                               Individual5, Individual6, Individual7, Individual8)

groups <- apply(groups, 1, as.list)

print(groups)

# and let's use this function from the asnipe package to create a matrix:

GBI <- get_group_by_individual(groups,
                               data_format="groups")

GBI
```
